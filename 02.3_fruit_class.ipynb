{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "### *Practicum AI:* Transfer - Fruit Classification with Transfer Learning\n",
    "\n",
    "This exercise was adapted from Baig et al. (2020) <i> The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Activity 3.02, page 150).\n",
    "\n",
    "(10 Minutes)\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "In this exercise, we will train a CNN to recognize 120 different kinds of fruit, using the [Fruits 360 dataset](https://arxiv.org/abs/1712.00580).  Horea Muresea and Mihai Oltean first shared this dataset in their article:  *Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol.10, Issue 1, pp.26-42, 2018*.\n",
    "\n",
    "The Fruits 360 datast contains 90,483 photos of 131 fruits and vegetables.  But rather than use the entire dataset, we use a subset of images - 16,000 photos of 120 different kinds of fruit - in this learning experience.  \n",
    "\n",
    "#### 1. Import requisite libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import the dataset and unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset and save the results to a variable called **zip_dir**.  The `tf.keras.get_file()` function does all this in a single statement.\n",
    "\n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> If you downloaded the dataset more than once, subsequent downloads will be slow as the get_file() function updates existing images one at a time.  The function works but is incredibly inefficient.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip\n",
      "82223104/82220233 [==============================] - 2s 0us/step\n",
      "82231296/82220233 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "zip_dir = tf.keras.utils.get_file('fruits360.zip', \n",
    "                                   origin       = file_url, \n",
    "                                   extract      = True, \n",
    "                                   cache_dir    = '/blue/rc-workshops/danielmaxwell',\n",
    "                                   cache_subdir = 'keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prepare the data for training\n",
    "\n",
    "Create a variable named **path** that contains the full path to the fruits360_filtered directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/rc-workshops/danielmaxwell/keras/fruits360_filtered\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the variables **train_dir** and **validation_dir**, with full paths to the train and validation folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = path / 'Training'\n",
    "validation_dir = path / 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of images for the train and validation datasets to 11398 and 4752 respectively.\n",
    "\n",
    "```python\n",
    "total_train = 11398\n",
    "total_val = 4752\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two image data generators using class `ImageDataGenerator`, one for train and a second for validation.  Augment the training data by rescaling, rotating, shifting, and flipping the images.  Validation data, though, only needs to be rescaled.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_image_generator = ImageDataGenerator(rescale = 1./255, rotation_range = 40, width_shift_range = 0.1, height_shift_range = 0.1, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "validation_image_generator = ImageDataGenerator(rescale = 1./255)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define four variables (**batch_size**, **img_height**, **img_width**, **channel**) and assign values to each.\n",
    "\n",
    "```python\n",
    "batch_size = 16\n",
    "img_height = 100\n",
    "img_width  = 100\n",
    "channel    = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train and validation data generators using `.flow_from_directory()`.  This step links the two image generators to actual data in a directory.  Also, set the batch_size and target_size arguments.\n",
    "\n",
    "```python\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size  = batch_size, directory = train_dir, target_size = (img_height, img_width))\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size  = batch_size, directory = validation_dir, target_size = (img_height, img_width))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the pre-trained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed values to 8 using `np.random_seed()` and `tf.random.set_seed()`.  This allows others to reproduce your results.\n",
    "\n",
    "```python\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained model VGG16 from `tensorflow.keras.applications`.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable called **base_model** and populate it with the following parameters for the VGG16 model.\n",
    "\n",
    "```python\n",
    "base_model = VGG16(input_shape = (img_height, img_width, channel), weights = 'imagenet', include_top = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Freeze all the layers in the base VGG16 model\n",
    "\n",
    "Set the model to non-trainable using the `.trainable` attribute.\n",
    "\n",
    "```python\n",
    "base_model.trainable =  False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary of the VGG16 model.\n",
    "\n",
    "```python\n",
    "base_model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the VGG16 model has 14,714,688 total parameters while the trainable parameters are zero. This is the case because we just froze all the layers of the VGG16 model.\n",
    "\n",
    "#### 6. Add custom classification layers\n",
    "\n",
    "Create a new model using `tf.kera.Sequential()` by adding the following layers to the base model:\n",
    "* A Flatten layer.\n",
    "* A Dense layer with 1000 units and ReLU activation.\n",
    "* The final Dense layer with 120 neurons and softmax activation.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1000, activation = 'relu'),\n",
    "    layers.Dense(120, activation = 'softmax')\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Compile the model for training\n",
    "\n",
    "Compile the model with **a)** an Adam optimizer, **b)** a learning rate of 0.001, **c)** the categorical crossentropy loss function, and **d)** an accuracy metric.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Train the Model\n",
    "Train the model using the `.fit()` method.  Provide the train and validation data generators, the steps per epoch, the total train epochs, and the validation steps.\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch  = total_train // batch_size,\n",
    "    epochs = 5,\n",
    "    validation_data  = val_data_gen,\n",
    "    validation_steps = total_val // batch_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "In this notebook, we used transfer learning to customize a pre-trained VGG16 model to classify images from the [Fruits 360 dataset](https://arxiv.org/abs/1712.00580). We replaced the head of the model with our own fully connected layers and then trained it for five epochs.  \n",
    "\n",
    "The accuracy scores for both training and validation are quite remarkable.  But can you do better?  What might you do differently?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
