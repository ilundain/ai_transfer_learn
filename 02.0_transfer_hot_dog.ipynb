{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "# *Practicum AI:* Keras Transfer Learning\n",
    "This exercise adapted from Krohn, J. and Beyleveld, G (2020) <i>Deep Learning Illustrated: A Visual, Interactive Guide to Artificial Intelligence </i> from <a href=\"https://github.com/the-deep-learners/deep-learning-illustrated/blob/master/notebooks/transfer_learning_in_keras.ipynb\">Pearson Education</a> (Transfer Learning, Chapter 10).\n",
    "\n",
    "In this notebook, we cover how to load a pre-trained model (VGGNet19) and finetune it for a new task: detecting hot dogs.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/hot_dog.png\" width=\"250\" height=\"250\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies\n",
    "\n",
    "Import the supporting libraries and VGG19 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pre-trained VGG19 model\n",
    "\n",
    "After the success of Alexnet in the 2012 ILSVRC competition, more complex models soon appeared.  VGGNet, for example, was the runner-up in the 2014 competition.  It has the classic convolution / pooling architecture of Alex but with more layers.\n",
    "\n",
    "- **include_top = false** : We do not want the final dense classification layers from the original model.  These layers were trained to classify the original ImageNet data.\n",
    "- **weights = 'imagenet'** : Use the model parameters trained on the 14 million-sample ImageNet dataset.\n",
    "- **input_shape = (224, 224, 3)** : Initialize the model with the input image size of our hot dog / not hot dog dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg19 = VGG19(include_top = False,\n",
    "              weights     = 'imagenet',\n",
    "              input_shape = (224,224,3),\n",
    "              pooling     = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze all the layers in the base VGGNet19 model\n",
    "\n",
    "Traverse the layers in the model and set the trainable flag to False. This freezes these layers and prevents them from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Documents...\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom classification layers\n",
    "\n",
    "Add a new *Dense* layer on top of the original VGG19 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the sequential model and add the VGG19 model: \n",
    "model = Sequential()\n",
    "model.add(vgg19)\n",
    "\n",
    "# Add the custom layers atop the VGG19 model: \n",
    "model.add(Flatten(name = 'flattened'))\n",
    "model.add(Dropout(0.5, name = 'dropout'))\n",
    "model.add(Dense(2, activation = 'softmax', name = 'predictions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model for training\n",
    "\n",
    "Compile the model with the Adaptive Momentum (adam) optimizer and the categorical_crossentropy loss function.  This loss function is appropriate for multi-class classification tasks where there are two or more output labels.  In this case, the label is either *hot dog* or *not hot dog*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire the dataset from this [location](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog/home).  And then define two data generator classes, one for training and a second for validation  The train data generator augments the data by rotating images within a 30 degree range, randomly flipping the images horizontally, rescaling the data by multiplying it by 1/255, and loading the image data into arrays in the channels_last format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate two image generator classes:\n",
    "train_datagen       = ImageDataGenerator(\n",
    "    rescale         = 1.0/255,\n",
    "    data_format     = 'channels_last',\n",
    "    rotation_range  = 30,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode       = 'reflect')\n",
    "\n",
    "valid_datagen   = ImageDataGenerator(\n",
    "    rescale     = 1.0/255,\n",
    "    data_format = 'channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the batch size:\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `flow_from_directory` method directs each generator to load images from the respective train and test directories.  Also, we specify that the data is categorical, divided into two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 499 images belonging to 2 classes.\n",
      "Found 500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define the train and validation generators: \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory   = './data/train',\n",
    "    target_size = (224, 224),\n",
    "    classes     = ['hot_dog','not_hot_dog'],\n",
    "    class_mode  = 'categorical',\n",
    "    batch_size  = batch_size,\n",
    "    shuffle     = True,\n",
    "    seed        = 42)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory   = './data/test',\n",
    "    target_size = (224, 224),\n",
    "    classes     = ['hot_dog','not_hot_dog'],\n",
    "    class_mode  = 'categorical',\n",
    "    batch_size  = batch_size,\n",
    "    shuffle     = True,\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "Now we train the model using the *fit_generator* method.  We use *fit_generator* instead of *fit* because the data is coming from data generators rather than arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/47771489/ipykernel_23690/2169329564.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=15,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:38:36.652277: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n",
      "2022-09-22 16:38:37.586112: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Running ptxas --version returned 32512\n",
      "2022-09-22 16:38:37.693303: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/15 [=>............................] - ETA: 59s - loss: 0.9510 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:38:39.339485: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 12s 543ms/step - loss: 0.9994 - accuracy: 0.5503 - val_loss: 0.7683 - val_accuracy: 0.5979\n",
      "Epoch 2/16\n",
      "15/15 [==============================] - 7s 453ms/step - loss: 0.5931 - accuracy: 0.7173 - val_loss: 0.4672 - val_accuracy: 0.7750\n",
      "Epoch 3/16\n",
      "15/15 [==============================] - 7s 455ms/step - loss: 0.4813 - accuracy: 0.7580 - val_loss: 0.5778 - val_accuracy: 0.7188\n",
      "Epoch 4/16\n",
      "15/15 [==============================] - 7s 481ms/step - loss: 0.3919 - accuracy: 0.8180 - val_loss: 0.4226 - val_accuracy: 0.7958\n",
      "Epoch 5/16\n",
      "15/15 [==============================] - 7s 458ms/step - loss: 0.4212 - accuracy: 0.7987 - val_loss: 0.6792 - val_accuracy: 0.7104\n",
      "Epoch 6/16\n",
      "15/15 [==============================] - 7s 455ms/step - loss: 0.4130 - accuracy: 0.8180 - val_loss: 0.5876 - val_accuracy: 0.7521\n",
      "Epoch 7/16\n",
      "15/15 [==============================] - 7s 453ms/step - loss: 0.3869 - accuracy: 0.8266 - val_loss: 0.4716 - val_accuracy: 0.7896\n",
      "Epoch 8/16\n",
      "15/15 [==============================] - 7s 473ms/step - loss: 0.3193 - accuracy: 0.8501 - val_loss: 0.5423 - val_accuracy: 0.7542\n",
      "Epoch 9/16\n",
      "15/15 [==============================] - 7s 448ms/step - loss: 0.2948 - accuracy: 0.8651 - val_loss: 0.5322 - val_accuracy: 0.7812\n",
      "Epoch 10/16\n",
      "15/15 [==============================] - 7s 462ms/step - loss: 0.2547 - accuracy: 0.8865 - val_loss: 0.5256 - val_accuracy: 0.7833\n",
      "Epoch 11/16\n",
      "15/15 [==============================] - 7s 449ms/step - loss: 0.2911 - accuracy: 0.8672 - val_loss: 0.4448 - val_accuracy: 0.7937\n",
      "Epoch 12/16\n",
      "15/15 [==============================] - 7s 454ms/step - loss: 0.2410 - accuracy: 0.8951 - val_loss: 0.5277 - val_accuracy: 0.7833\n",
      "Epoch 13/16\n",
      "15/15 [==============================] - 7s 449ms/step - loss: 0.2386 - accuracy: 0.8994 - val_loss: 0.5870 - val_accuracy: 0.7479\n",
      "Epoch 14/16\n",
      "15/15 [==============================] - 7s 450ms/step - loss: 0.2592 - accuracy: 0.8972 - val_loss: 0.4876 - val_accuracy: 0.7937\n",
      "Epoch 15/16\n",
      "15/15 [==============================] - 7s 454ms/step - loss: 0.2295 - accuracy: 0.9058 - val_loss: 0.5167 - val_accuracy: 0.7771\n",
      "Epoch 16/16\n",
      "15/15 [==============================] - 7s 450ms/step - loss: 0.2610 - accuracy: 0.8929 - val_loss: 0.7810 - val_accuracy: 0.7188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2baa5e442fa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch = 15, \n",
    "                    epochs = 16, validation_data = valid_generator, \n",
    "                    validation_steps = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
